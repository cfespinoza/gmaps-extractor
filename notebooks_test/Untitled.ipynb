{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n",
      "/home/cflores/cflores_workspace/cflores_venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping statistics from Yahoo Finance by company name\n",
      "Scraping Wikipedia table in python : common grid style\n",
      "Scraping a wikipedia table with Rvest (error)\n",
      "Web Scraping Cleansing\n",
      "Where to find regional/global GitHub website statistics [closed]\n",
      "Scrapy Amazon Search and Scrape Multiple products\n",
      "BeautifulSoup Python web scraping Missing html Main Body\n",
      "Using BeautifulSoup to Web Scrape Survivor Page Wikipedia\n",
      "Web Scraping with Beautifulsoup: Cannot find class\n",
      "How to get data from javascript rendered table using selenium in python\n",
      "Scraping GIS coordinates from a non-traditional map using selenium?\n",
      "how to get data from inside the div tag with xpath or css like [closed]\n",
      "How to parse a downloadable list from a web site?\n",
      "Get details from JSON response by sending http get request\n",
      "Unable to go from on page to another using scrapy\n",
      "Warning: unstable_flushDiscreteUpdates\n",
      "Anyone know where I can download or scrape weekly NFL depth charts (offense and defense) and starting lineups in CSV form? [closed]\n",
      "How can I fix this python code to get all of the names of the people in the table I am referencing in the code?\n",
      "Follow up question: uneven html leads to imbalance in python list\n",
      "Extract List Values using Beautiful Soup in Python\n",
      "Why is my scraping code not copying a table from a webpage?\n",
      "UnhandledPromiseRejectionWarning: Unhandled promise rejection while running puppeteer script\n",
      "Scraping stock price from Yahoo Finance using Python & BeautifulSoup\n",
      "Getting response of a form request using scrapy\n",
      "My link gets visited 15 times directly after posting on twitter from scrapers(I think) [closed]\n",
      "What are some different ways that you obtain metadata from SoundCloud after the API was inaccessible?\n",
      "How do I extract chart back end data from website?\n",
      "How to extract data from a google spreadsheet to notebook\n",
      "Set Proxy for chrome web driver in selenium Python\n",
      "How to open multiple page more efficiently?\n",
      "Issue scraping website with bs4 (beautiful soup) python 2.7\n",
      "How to “webscrape” a site containing a popup window, using python?\n",
      "What would be the best way to scrape this website? (Not Selenium)\n",
      "How to run a Java program to periodically scrape web data [closed]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2010' in position 71959: character maps to <undefined>\n",
      "How to get Javascript generated content\n",
      "I need to web scraping data from a website that uses frames\n",
      "How do I interact with the HTML of a session (created with the requests module)?\n",
      "Web Crawl in R with Tidyverse/Rvest_implement a for loop to extract all links from multiple index pages\n",
      "BeautifulSoup web scrape get children\n",
      "Interact with all elements in a list in R\n",
      "how to retrieve nodejs scraped data and display in html\n",
      "How to automatically update HTML daily [closed]\n",
      "Beautiful Soup will not return anything legible\n",
      "Web Scraping Image URL Selenium\n",
      "how to visualize page_source returned from selenium\n",
      "Web Scraping News Articles\n",
      "Why isn't this function to scrape a table with Selenium working as intended?\n",
      "Make a flask, selenium, system works?\n",
      "Selenium Webdriver NoSuchElementException on python\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from multiprocessing.pool import ThreadPool, Pool\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import threading\n",
    "\n",
    "def get_links(link):\n",
    "  res = requests.get(link)\n",
    "  soup = BeautifulSoup(res.text,\"lxml\")\n",
    "  titles = [urljoin(url,items.get(\"href\")) for items in soup.select(\".summary .question-hyperlink\")]\n",
    "  return titles\n",
    "\n",
    "threadLocal = threading.local()\n",
    "\n",
    "def get_driver():\n",
    "  driver = getattr(threadLocal, 'driver', None)\n",
    "  if driver is None:\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "    chromeOptions.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(chrome_options=chromeOptions, \n",
    "                              executable_path=\"/home/cflores/cflores_workspace/gmaps-extractor/resources/chromedriver\")\n",
    "    setattr(threadLocal, 'driver', driver)\n",
    "  return driver\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "  driver = get_driver()\n",
    "  driver.get(url)\n",
    "  sauce = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "  item = sauce.select_one(\"h1 a\").text\n",
    "  print(item)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  url = \"https://stackoverflow.com/questions/tagged/web-scraping\"\n",
    "  Pool(10).map(get_title,get_links(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
